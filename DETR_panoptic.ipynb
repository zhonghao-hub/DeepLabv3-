{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzkJUMTxn8hB"
   },
   "source": [
    "# Panoptic segmentation using DETR\n",
    "\n",
    "In this notebook we demonstrate how to explore the panoptic segmentation capabilities of DETR. The prediction occurs in several steps: \n",
    "\n",
    "1.   The model predicts a box and a binary mask for each object queries\n",
    "2.   We filter the predictions for which the confidence is < 85%\n",
    "3.   Finally, the remaining masks are merged together using a pixel-wise argmax\n",
    "\n",
    "For simplicity, we rely on DETR's postprocessor to execute 2 and 3. We encourage to take a look at the corresponding code to get a better understanding of the process.\n",
    "\n",
    "Finally, we visualize the final prediction using Detectron2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D94dCpHipOx6"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wObDPOY2poRT"
   },
   "source": [
    "This section contains the necessary boiler-plate. Run it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WCXaQ4VaJXv0",
    "outputId": "d8371cd0-bc89-440c-a5a8-fdf6d8a681ec"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as D\n",
    "import numpy as np\n",
    "torch.set_grad_enabled(False);\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "import panopticapi\n",
    "from panopticapi.utils import id2rgb, rgb2id\n",
    "\n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QD4mQxHIqGCr"
   },
   "outputs": [],
   "source": [
    "# These are the COCO classes\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n",
    "\n",
    "# Detectron2 uses a different numbering scheme, we build a conversion table\n",
    "coco2d2 = {}\n",
    "count = 0\n",
    "for i, c in enumerate(CLASSES):\n",
    "  if c != \"N/A\":\n",
    "    coco2d2[i] = count\n",
    "    count+=1\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "#     T.Resize(1333),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMFPx33oqjl-"
   },
   "source": [
    "## Using a model from hub\n",
    "\n",
    "We load a pre-trained model directly from torch hub. Note that we also request the post-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5uUW5iRnJhxM",
    "outputId": "1c85c6d6-0ed5-4375-e7a1-585d2d552c94"
   },
   "outputs": [],
   "source": [
    "model, postprocessor = torch.hub.load('facebookresearch/detr', 'detr_resnet101_panoptic', pretrained=True, return_postprocessor=True, num_classes=250)\n",
    "model.eval();\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocODwfQCrdMJ"
   },
   "source": [
    "Next, we retrieve an image on which we wish to test the model. Here, we use an image from the validation set of COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "INPUT_FOLDER = '/home/msmith/localDrive/DJI/Flying - Marlington Rd./videoasjpg/'\n",
    "dataset = D.ImageFolder(INPUT_FOLDER, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "           pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directory\n",
    "OUT_DIR = '/home/msmith/localDrive/DJI/Flying - Marlington Rd./modeloutjpg/'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# Go over all images, run model, get results\n",
    "index = 0\n",
    "for data in tqdm(dataloader):\n",
    "    \n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    img = data[0]\n",
    "    out = model(img.cuda())\n",
    "    \n",
    "    # the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
    "    results = postprocessor(out, [torch.as_tensor(img.shape[-2:]) for i in range(batch_size)])\n",
    "    \n",
    "    for res in results:\n",
    "        # We extract the segments info and the panoptic result from DETR's prediction\n",
    "        segments_info = deepcopy(res[\"segments_info\"])\n",
    "        # Panoptic predictions are stored in a special format png\n",
    "        panoptic_seg = Image.open(io.BytesIO(res['png_string']))\n",
    "        final_w, final_h = panoptic_seg.size\n",
    "        # We convert the png into an segment id map\n",
    "        panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)\n",
    "        panoptic_seg = torch.from_numpy(rgb2id(panoptic_seg))\n",
    "\n",
    "        # Detectron2 uses a different numbering of coco classes, here we convert the class ids accordingly\n",
    "        meta = MetadataCatalog.get(\"coco_2017_val_panoptic_separated\")\n",
    "        for i in range(len(segments_info)):\n",
    "            c = segments_info[i][\"category_id\"]\n",
    "            segments_info[i][\"category_id\"] = meta.thing_dataset_id_to_contiguous_id[c] if segments_info[i][\"isthing\"] else meta.stuff_dataset_id_to_contiguous_id[c]\n",
    "\n",
    "        # Finally we visualize the prediction\n",
    "        v = Visualizer(np.array(Image.open(dataset.imgs[index][0]).resize((final_w, final_h)))[:, :, ::-1], meta, scale=1.0)\n",
    "        v._default_font_size = 20\n",
    "        v = v.draw_panoptic_seg_predictions(panoptic_seg, segments_info, area_threshold=0)\n",
    "        visualization = Image.fromarray(v.get_image())\n",
    "        visualization.save(os.path.join(OUT_DIR, 'vis_%04d.jpg' % index))\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsnJrhu2J_zc"
   },
   "outputs": [],
   "source": [
    "# url = \"http://images.cocodataset.org/val2017/000000281759.jpg\"\n",
    "# im = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "is76xRRYsC3y"
   },
   "source": [
    "This returns a mask for each query, let us visualize the high confidence ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "colab_type": "code",
    "id": "-5ytUV_qsVhL",
    "outputId": "a27bd5a8-619f-486d-9cb6-c39e0f9a8eb3"
   },
   "outputs": [],
   "source": [
    "# compute the scores, excluding the \"no-object\" class (the last one)\n",
    "scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
    "# threshold the confidence\n",
    "keep = scores > 0.85\n",
    "\n",
    "# Plot all the remaining masks\n",
    "ncols = 5\n",
    "fig, axs = plt.subplots(ncols=ncols, nrows=math.ceil(keep.sum().item() / ncols), figsize=(18, 10))\n",
    "for line in axs:\n",
    "    for a in line:\n",
    "        a.axis('off')\n",
    "for i, mask in enumerate(out[\"pred_masks\"][keep]):\n",
    "    ax = axs[i // ncols, i % ncols]\n",
    "    ax.imshow(mask, cmap=\"cividis\")\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjDRLX3yDrlH"
   },
   "source": [
    "Now that we have the individual masks, we can merge the predictions into a unified panoptic segmentation. We use DETR's postprocessor for that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "LurQI5Z-Ay1Z",
    "outputId": "d2e36e12-c084-4ec3-8bac-8c568e6692ae"
   },
   "outputs": [],
   "source": [
    "# the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
    "result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKxteXUKvQZ-"
   },
   "source": [
    "## Panoptic visualization using Detectron2\n",
    "\n",
    "In this section we demonstrate how to obtain a better looking visualization by leveraging Detectron2's plotting utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEZkdoc-wC7m"
   },
   "source": [
    "Finally, we visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "colab_type": "code",
    "id": "rYoKWqFyBWE9",
    "outputId": "9a96b199-7de7-4186-bb04-1a1cbbb1e334"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# We extract the segments info and the panoptic result from DETR's prediction\n",
    "segments_info = deepcopy(result[\"segments_info\"])\n",
    "# Panoptic predictions are stored in a special format png\n",
    "panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n",
    "final_w, final_h = panoptic_seg.size\n",
    "# We convert the png into an segment id map\n",
    "panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)\n",
    "panoptic_seg = torch.from_numpy(rgb2id(panoptic_seg))\n",
    "\n",
    "    \n",
    "    \n",
    "# Detectron2 uses a different numbering of coco classes, here we convert the class ids accordingly\n",
    "meta = MetadataCatalog.get(\"coco_2017_val_panoptic_separated\")\n",
    "for i in range(len(segments_info)):\n",
    "    c = segments_info[i][\"category_id\"]\n",
    "    segments_info[i][\"category_id\"] = meta.thing_dataset_id_to_contiguous_id[c] if segments_info[i][\"isthing\"] else meta.stuff_dataset_id_to_contiguous_id[c]\n",
    "\n",
    "\n",
    "# Finally we visualize the prediction\n",
    "v = Visualizer(numpy.array(im.copy().resize((final_w, final_h)))[:, :, ::-1], meta, scale=1.0)\n",
    "v._default_font_size = 20\n",
    "v = v.draw_panoptic_seg_predictions(panoptic_seg, segments_info, area_threshold=0)\n",
    "Image.fromarray(v.get_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of DETR_panoptic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
